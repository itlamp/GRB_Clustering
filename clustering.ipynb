{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering GRB's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from glob import glob\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pca_components = 20\n",
    "dbscan_eps = 2.5\n",
    "dbscan_min_samples = 2\n",
    "num_kmeans_clusters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_number(filename):\n",
    "    # Extract the number between \"bn\" and \"_v00\" using regular expression\n",
    "    match = re.search(r'(\\d+).npy', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None  # Return None for filenames that don't match the pattern\n",
    "    \n",
    "files = glob('../clean_bursts/*')\n",
    "data_list = []\n",
    "data_list_names = []\n",
    "for file in files:\n",
    "    data_list.append(np.load(file))\n",
    "    data_list_names.append(extract_number(file))\n",
    "\n",
    "max_length = max(len(arr) for arr in data_list)\n",
    "data_list = [(arr-min(arr))/max(arr-min(arr)) for arr in data_list]\n",
    "data_list = [np.pad(arr, (0, max_length - len(arr)), mode='minimum') for arr in data_list]\n",
    "\n",
    "light_curves = np.stack(data_list)\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# light_curves_standardized = scaler.fit_transform(light_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=num_pca_components)\n",
    "X_pca = pca.fit_transform(light_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize DBSCAN model\n",
    "dbscan = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples)\n",
    "\n",
    "# Fit the model to the standardized light curves\n",
    "cluster_assignments_dbscan = dbscan.fit_predict(X_pca)\n",
    "\n",
    "total_curves = 0\n",
    "count_curves = {}\n",
    "for i in set(cluster_assignments_dbscan):\n",
    "    num = sum(cluster_assignments_dbscan==i)\n",
    "    print(f'{num} in cluster {i}')\n",
    "    count_curves[num] = count_curves.get(num, 0) + 1\n",
    "    total_curves += num\n",
    "\n",
    "print(f'total number of curves is {total_curves}')\n",
    "for num in count_curves.keys():\n",
    "    print(f'there are {count_curves[num]} clusters with {num} elements')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices_dict = {}\n",
    "num_show = 10\n",
    "\n",
    "for num in np.unique(cluster_assignments_dbscan):\n",
    "    indices = np.where(cluster_assignments_dbscan == num)[0]\n",
    "    stop = max(num_show, len(indices))\n",
    "    indices_dict[num] = indices[:num_show]\n",
    "\n",
    "indices_dict.pop(-1)\n",
    "\n",
    "\n",
    "for clusters in indices_dict.keys():\n",
    "    plt.figure(figsize=(9, 3))\n",
    "    plt.suptitle(f\"cluster {clusters}\", fontsize=18, y=0.95)\n",
    "\n",
    "    num_cols = int(np.ceil(len(indices_dict[clusters]) / 2 ))\n",
    "    for i, indices in enumerate(indices_dict[clusters], 1):\n",
    "        ax = plt.subplot(2,num_cols, i)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.plot(np. trim_zeros(light_curves[indices,:]), 'b')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DBSCAN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_dict = {}\n",
    "for name, cluster in zip(data_list_names, cluster_assignments_dbscan):\n",
    "    if cluster in clust_dict:\n",
    "        clust_dict[cluster].append(name)\n",
    "    else: \n",
    "        clust_dict.update({cluster: [name]})\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({k: pd.Series(v) for k, v in clust_dict.items()})\n",
    "df.to_excel('dbscan_clustering.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = num_kmeans_clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Fit the model to the standardized light curves\n",
    "cluster_assignments_kmeans = kmeans.fit_predict(X_pca)\n",
    "  \n",
    "\n",
    "total_curves = 0\n",
    "count_curves = {}\n",
    "for i in set(cluster_assignments_kmeans):\n",
    "    num = sum(cluster_assignments_kmeans==i)\n",
    "    print(f'{num} in cluster {i}')\n",
    "    count_curves[num] = count_curves.get(num, 0) + 1\n",
    "    total_curves += num\n",
    "\n",
    "print(f'total number of curves is {total_curves}')\n",
    "for num in count_curves.keys():\n",
    "    print(f'there are {count_curves[num]} clusters with {num} elements')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Kmeans results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices_dict = {}\n",
    "num_show = 10\n",
    "\n",
    "for num in np.unique(cluster_assignments_kmeans):\n",
    "    indices = np.where(cluster_assignments_kmeans == num)[0]\n",
    "    stop = max(num_show, len(indices))\n",
    "    indices_dict[num] = indices[:num_show]\n",
    "\n",
    "\n",
    "for clusters in indices_dict.keys():\n",
    "    plt.figure(figsize=(9, 3))\n",
    "    plt.suptitle(f\"cluster {clusters}\", fontsize=18, y=0.95)\n",
    "\n",
    "    num_cols = int(np.ceil(len(indices_dict[clusters]) / 2 ))\n",
    "    for i, indices in enumerate(indices_dict[clusters], 1):\n",
    "        ax = plt.subplot(2,num_cols, i)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.plot(np. trim_zeros(light_curves[indices,:]), 'b')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# indices_dict = {}\n",
    "# num_show = 3\n",
    "\n",
    "# for num in np.unique(cluster_assignments_kmeans):\n",
    "#     indices = np.where(cluster_assignments_kmeans == num)[0]\n",
    "#     stop = max(num_show, len(indices))\n",
    "#     indices_dict[num] = indices[:num_show]\n",
    "\n",
    "# for clusters in indices_dict.keys():\n",
    "#     for indices in indices_dict[clusters]:\n",
    "#         plt.plot(light_curves[indices,:])\n",
    "#         plt.title(f'cluster {clusters}')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Kmeans results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_dict = {}\n",
    "for name, cluster in zip(data_list_names, cluster_assignments_kmeans):\n",
    "    if cluster in clust_dict:\n",
    "        clust_dict[cluster].append(name)\n",
    "    else: \n",
    "        clust_dict.update({cluster: [name]})\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({k: pd.Series(v) for k, v in clust_dict.items()})\n",
    "df.to_excel('kmeans_clustering.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find ideal k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for k in tqdm(range(1,150)):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_assignments_kmeans = kmeans.fit_predict(X_pca)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(np.array(inertias))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(light_curves)\n",
    "\n",
    "color_neg_one= {-1: 'black'}\n",
    "color_palete = sns.husl_palette(n_colors=40, s=0.7, l=0.6)\n",
    "color_palete = [color_neg_one.get(group, color) for group, color in zip(range(-1, 39), color_palete)]\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:,0], y=tsne_results[:,1],\n",
    "    palette=color_palete,\n",
    "    hue = cluster_assignments_dbscan,\n",
    "    data=tsne_results,\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:,0], y=tsne_results[:,1],\n",
    "    palette=sns.husl_palette(n_colors=40, s=0.7, l=0.6),\n",
    "    hue = cluster_assignments_kmeans,\n",
    "    data=tsne_results,\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
